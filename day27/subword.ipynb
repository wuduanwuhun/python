{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T03:18:39.889707Z",
     "start_time": "2025-01-24T03:18:37.456245Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:21:25.672321Z",
     "iopub.status.busy": "2025-01-24T14:21:25.671964Z",
     "iopub.status.idle": "2025-01-24T14:21:28.897476Z",
     "shell.execute_reply": "2025-01-24T14:21:28.896630Z",
     "shell.execute_reply.started": "2025-01-24T14:21:25.672289Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=10, micro=14, releaselevel='final', serial=0)\n",
      "matplotlib 3.10.0\n",
      "numpy 1.26.4\n",
      "pandas 2.2.3\n",
      "sklearn 1.6.0\n",
      "torch 2.5.1+cu124\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, torch:\n",
    "    print(module.__name__, module.__version__)\n",
    "    \n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据\n",
    "\n",
    "这里使用subword分词，我们使用已经清洗好的数据集，可以从[此处](https://www.kaggleusercontent.com/kf/98352223/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..nKt8lrIW5ej5QJQVpOWuqQ.oLIgiLMONU5Gpj_maVudRJa55NSOCILxk4JNZhvuXmeDBR-oG0uQm7bDHBfSwZRGvOBHQTsRV308iNP80btfwMinQ7yvJNt-GwdQF4XR4DIsg-2CbEPYiMsi_NdbL0FmE9LYStKdxCWbrCZCCMrTmo5LxR1txwibXaSpeP5Inobhbez5zetZIRH210CBuX2JbpRc_DULQpazKbtFPitwyfktVmdG_syvVAU6Sk9b0r0_erYAgb_jkKXX1Mxo1KzWSKLcAvbmMIPcsUkx9PmeJDs_wopfsQsZ1h5jaQX4_l0CTZrEenP6lIPDxpTwXANqqdHspmZeeEIAThqCHC6sb5DxTvG89BwzY9rc53Aa0uX4V806wJVybnRXoaV65K4GqpjnxbBK0WC8G-2lNtrqFujE89KDXZjFPgyfOEj1QIu13oFNSjgs6o4VV1PdZOrhiNdSmjb44c22l_unOaFojzJgzcPxq9AG2lcmrOpdZ2qu1jjdwey-58TA2ZHNCo3XnjEe2n3ignpnbsdLFpo22O8QakSUHv91wuYDYdNi3AiSmltL_k2ChuKfJ0G8kATpLe4k8wA26sO4GMXg4HImOr3b4aDVEIWXdApHP0ecFKs6ELTo8O7X-TK8Jvbua7e6qpDfDc-r_cD73fVSgSek5yNmKQMBzuVcjkprXmcxICQ.kV1b4N1s64NERKnt4zwQgQ/imdb_processed.csv)下载，分词使用 [subword-nmt](https://github.com/rsennrich/subword-nmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:22:27.858055Z",
     "start_time": "2025-01-24T06:22:27.436613Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:21:28.900289Z",
     "iopub.status.busy": "2025-01-24T14:21:28.899204Z",
     "iopub.status.idle": "2025-01-24T14:21:29.450039Z",
     "shell.execute_reply": "2025-01-24T14:21:29.449043Z",
     "shell.execute_reply.started": "2025-01-24T14:21:28.900221Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = pd.read_csv(\"imdb_processed.csv\")\n",
    "print(cleaned_df.shape) # (50000, 2), 50000条评论, 2列\n",
    "\n",
    "# 随机打乱数据，取训练集和测试集\n",
    "np.random.seed(seed) #随机\n",
    "cleaned_df = cleaned_df.sample(frac=1).reset_index(drop=True)#打乱，frac=1表示全部打乱（frac是比例，reset_index(drop=True)是重新索引\n",
    "with open(\"imdb_train.txt\", \"w\", encoding=\"utf8\") as file:# 保存训练集\n",
    "    for line in cleaned_df.processed.values[:25000]:#只保存了processed列，即评论文本，没有保存label列\n",
    "        file.write(line.lower() + \"\\n\") #变为小写，token数量少一些\n",
    "\n",
    "with open(\"imdb_test.txt\", \"w\", encoding=\"utf8\") as file:# 保存测试集\n",
    "    for line in cleaned_df.processed.values[25000:]:\n",
    "        file.write(line.lower() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T06:14:08.224959500Z",
     "start_time": "2024-05-02T06:14:08.190978500Z"
    },
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:21:29.452139Z",
     "iopub.status.busy": "2025-01-24T14:21:29.451469Z",
     "iopub.status.idle": "2025-01-24T14:21:34.955541Z",
     "shell.execute_reply": "2025-01-24T14:21:34.954414Z",
     "shell.execute_reply.started": "2025-01-24T14:21:29.452089Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.10/site-packages (0.3.8)\n",
      "Requirement already satisfied: mock in /usr/local/lib/python3.10/site-packages (from subword-nmt) (5.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from subword-nmt) (4.67.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install subword-nmt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:24:15.671594Z",
     "start_time": "2025-01-24T06:24:06.369793Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:21:34.957344Z",
     "iopub.status.busy": "2025-01-24T14:21:34.956893Z",
     "iopub.status.idle": "2025-01-24T14:21:59.539958Z",
     "shell.execute_reply": "2025-01-24T14:21:59.538859Z",
     "shell.execute_reply.started": "2025-01-24T14:21:34.957306Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|######################################| 8000/8000 [00:12<00:00, 663.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# 学习bpe分词(很慢,学一次就好)\n",
    "# -i 选择学习的文件\n",
    "# -o 核心输出文件,分词需要用到imdb_bpe_code\n",
    "# --write-vocabulary 字典输出文件\n",
    "# -s 词表大小\n",
    "!subword-nmt learn-joint-bpe-and-vocab -i ./imdb_train.txt -o ./imdb_bpe_code --write-vocabulary ./imdb_bpe_vocab -s 8000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:27:23.512416Z",
     "start_time": "2025-01-24T06:27:17.835322Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-24T14:21:59.541775Z",
     "iopub.status.busy": "2025-01-24T14:21:59.541341Z",
     "iopub.status.idle": "2025-01-24T14:22:15.781875Z",
     "shell.execute_reply": "2025-01-24T14:22:15.780546Z",
     "shell.execute_reply.started": "2025-01-24T14:21:59.541734Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 应用bpe分词,-c 指定 BPE 编码的配置文件\n",
    "!subword-nmt apply-bpe -c ./imdb_bpe_code -i ./imdb_train.txt -o ./imdb_train_bpe.txt\n",
    "!subword-nmt apply-bpe -c ./imdb_bpe_code -i ./imdb_test.txt -o ./imdb_test_bpe.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:27:28.092595Z",
     "start_time": "2025-01-24T06:27:27.981169Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:15.785412Z",
     "iopub.status.busy": "2025-01-24T14:22:15.784977Z",
     "iopub.status.idle": "2025-01-24T14:22:15.914237Z",
     "shell.execute_reply": "2025-01-24T14:22:15.913233Z",
     "shell.execute_reply.started": "2025-01-24T14:22:15.785373Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really liked sum@@ mer@@ sla@@ m due look a@@ ren@@ a , cur@@ tain look overall interesting reason . anyways , could one best sum@@ mer@@ sla@@ m ever w@@ w@@ f le@@ x lu@@ ger main event yo@@ ko@@ z@@ un@@ a , time ok huge fat man v strong man i glad time changed . it terrible main event like every match lu@@ ger terrible . other match card ra@@ z@@ or ra@@ mon v ted di@@ bi@@ ase , ste@@ in@@ er brothers v hea@@ ven@@ ly bo@@ dies , sha@@ wn micha@@ els v cur@@ t h@@ ening , event sha@@ wn named big monster body guard die@@ sel , irs v - - kid , bre@@ t hart first take do@@ ink take jerry law@@ ler stuff har@@ ts law@@ ler always interesting , lu@@ d@@ vi@@ g bor@@ ga destroyed marty jan@@ ne@@ tty , under@@ taker took giant gon@@ z@@ ale@@ z another terrible match , the smoking g@@ unn@@ s tat@@ an@@ ka took ba@@ m ba@@ m bi@@ ge@@ low head@@ shr@@ in@@ kers , yo@@ ko@@ z@@ un@@ a def@@ ended world title le@@ x lu@@ ger match boring terrible ending . however deserves\n",
      "not many television show appeal quite many different kind fan like far@@ scape . . . i know youn@@ g@@ ster year old fan male female many different country think ad@@ ore t . v miniseries . it element found almost every show t . v , character driven drama could australian soap opera yet episode science fact fiction would give even har@@ di@@ est tre@@ k@@ kie run money brain@@ ben@@ der sta@@ ke ! wor@@ m@@ hole theory , time travel true equ@@ a@@ tional form . . . magnificent . it embr@@ ace culture ma@@ p possibility endless multiple star therefore thousand planet choose . with broad scope would expected nothing would able keep illu@@ sion long , far@@ scape really come element . . . it succeeds others failed , especially like star trek universe practically zero ka@@ os element ! they ran idea pretty quickly kept re@@ ha@@ shing ! over course season manage keep audience attention using good continuity constant character evol@@ ution multiple thread every episode unique personal touch camera specific certain character group within whole . this structure allows extremely large area subject matter loy@@ alty for@@ ged broken many way many many issue . i happened see pilot premiere passing keep tun@@ ing see cri@@ ch@@ ton would ever get girl , seeing television i deli@@ ghted see available dvd i admit thing kept s@@ ane whilst i hour night shift developed chron@@ ic in@@ som@@ nia . . . far@@ scape thing get extremely long night . . . do fav@@ our watch pilot see i mean . . . far@@ scape com@@ et\n",
      "the film quickly get major chase scene ever increa@@ sing destruction . the first really bad thing guy hi@@ jac@@ king steven seagal would beaten pulp seagal driving , probably would ended whole premise movie . it seems like decided make kind change movie plot , plan enjoy action , expect coherent plot . turn sense logic may , re@@ duce chance getting head@@ ache . i give hope steven seagal trying move back towards type character portrayed popular movie .\n",
      "jane austen would definitely appro@@ ve one ! g@@ wy@@ ne@@ th pal@@ tro@@ w awesome job capturing attitude emma . she funny without exce@@ ssively silly , yet eleg@@ ant . she put convincing british accent british , maybe i best judge , fooled . . . also excellent sli@@ ding do@@ ors . . . i sometimes forget american ! . also brilliant jeremy north@@ am sophi@@ e th@@ omp@@ son ph@@ y@@ lli@@ da law emma th@@ omp@@ son sister mother bates woman . they nearly steal show . . . ms . law even line ! highly recommended .\n",
      "expec@@ ta@@ tions somewhat high i went see movie , i thought steve care@@ ll could wrong coming great movie like an@@ chor@@ man , the year - old virgin , little miss sun@@ shine . boy , i wrong . i start right movie certain point steve care@@ ll allowed steve care@@ ll . there handful moment film made laugh , due almost entirely given wi@@ ggle - room thing . he undoubtedly talented individual , shame sig@@ ned turned , opinion , total train - wreck . with way , i discus went horri@@ f@@ y@@ ingly wrong . the film begin dan burns , wi@@ do@@ wer three girl considered na@@ tionally syn@@ di@@ cated advice colum@@ n . he pre@@ pa@@ res girl family reunion , extended relative ga@@ ther time . the family high at@@ op list thing make awful movie . no family beha@@ ves like . it almost tran@@ spor@@ ted pleasan@@ t@@ ville leave bea@@ ver . they caricature think family . it reach point become obnoxious simply frustrating . touch football , cro@@ ss@@ word puzzle competition , family bow@@ ling , talent show are not how actual people behave . it almost sick@@ ening . another big flaw woman care@@ ll supposed falling . obser@@ ving first scene steve care@@ ll like watching stro@@ ke victim trying re@@ hab@@ il@@ it@@ ated . what i imagine supposed unique original woman come mildly retarded . it make think movie taking place another planet . i left theater wondering i saw . after thinking , i think much .\n",
      "i watched movie fairly regular basis life , never get old . for sni@@ de remark insult mostly david spa@@ de , tommy boy giant heart . and keep movie funny year . tommy cal@@ la@@ han chris far@@ ley son big tom cal@@ la@@ han brian den@@ ne@@ h@@ y , master car part sal@@ es@@ man , ri@@ dden life . but died dy wedding day , tommy learns company deb@@ t , bought ray z@@ al@@ in@@ sky dan ak@@ roy@@ d , owner huge car part company . so order save company , tommy go road sell company new bra@@ ke pa@@ d . along ride , though choice , richard hay@@ den david spa@@ de former cla@@ ss@@ mate tommy big tom right - hand man . the movie ride chemistry two sn@@ l star real - life best friend chris far@@ ley david spa@@ de . the duo enough comic energy going power world . it big , dumb guy versus smart little guy . it work , scene un@@ forget@@ tably funny . far@@ ley spa@@ de actually decent dramatic actor well . although film primarily comedy , fair share drama , spa@@ de especially far@@ ley good making audience laugh . forgive , i talk chris far@@ ley little . i read biography the chris far@@ ley show a biography three acts , anyone care , understanding chris real life made movie special . chris far@@ ley genuinely good person strugg@@ led , ultimately failed con@@ qu@@ er addic@@ tion . although first movie major role , best film . it really showed , much talent . knowing chris story add another layer movie , although make le funny . far@@ ley spa@@ de matched good screen cast . rob lo@@ we suit@@ ably sli@@ my tommy new brother , bo derek solid step - mother . brian den@@ ne@@ h@@ y great big tom . den@@ ne@@ h@@ y make easy believe father son . big tom crazy son , although smar@@ ter mature . dan ak@@ roy@@ d give one best performance z@@ al@@ in@@ sky , giving tommy hard truth behind advertising . julie warner also good tommy love interest , michelle . for , peter se@@ gal one great comedy director . he keep pace quick ener@@ ge@@ tic , importantly , know make comedy funny . he be@@ la@@ bor joke , understand@@ s funny actor know allows . but se@@ gal go step . he give tommy boy friendly , almost nostal@@ gic tone tu@@ g heart@@ str@@ ings genuinely ti@@ ckle f@@ unn@@ y@@ bone . cri@@ tics like tommy boy . shame . a movie super sophisticated sub@@ ver@@ sively intellectual funny god for@@ bid far@@ ley spa@@ de forced mu@@ ted comedy la the office . this great movie one - time favorite .\n",
      "for story hope high@@ ligh@@ ted tragic reality youth face . fav@@ ela rising draw one scary , un@@ safe un@@ fair world show beautiful color moving music one man dedicated friend choose accept world change action art . an entertaining , interesting , emotional , aes@@ the@@ tically beautiful film . i showed film numerous high school student well live neighborhood poverty gun violence en@@ amo@@ red anderson , protagonist . i recommend film age due subtitle image death background .\n",
      "okay , i get pur@@ gatory thing first time i watched episode . it seemed like something significant going i put finger . this time co@@ sta me@@ sa fire tv really caught attention - helped i writing ess@@ ay inf@@ er@@ no ! but let see ha@@ s@@ n t discu@@ ssed yet . . . a tw@@ op review mentioned tony flight sta@@ ir go broken elev@@ ator . yeah , significant number lot reason , especially religious , one ya . on hun@@ ch i con@@ sulted wi@@ ki@@ pe@@ dia , guess dan@@ te divi@@ ded level ? pur@@ g@@ at@@ ori@@ o . exclu@@ ding ante - pur@@ gatory para@@ di@@ se . the stuff bottom sta@@ ir . . . tony get . on alle@@ ge@@ dly random mon@@ k - slap scene . as soon mon@@ k appeared , fit perfectly place tony trying get pur@@ gatory . you tell got worried christian commercial death , disease , sin came , getting desperate christian heaven looking kinda i@@ ffy . by time meet mon@@ k thinking hey maybe guy help ? sound like contem@@ pla@@ ting religion e . g . bud@@ d@@ h@@ ism wondering path could take sal@@ vation . not tony necessarily literally thinking becoming bud@@ d@@ hi@@ st , appears fin@@ ner@@ ty tried me@@ ssed . that slap face basically tell tony quick fix - , , suddenly embr@@ ace bud@@ d@@ h@@ ism get . tony initially concerned getting heaven . but con@@ ference en@@ tr@@ ance , realizes going easy . at first i saw name v . driver li@@ cen@@ se problem tony led sort double life , killing people sleeping around kept secret people . he feel free affair qu@@ as@@ i - mel@@ fi kevin fin@@ ner@@ ty . he figure can fool people k@@ f card , like hotel rec@@ ep@@ tion@@ ist , get pur@@ gatory . those helicopter - helicopter heaven ? - keeping track everything . after reading theory in@@ fin@@ ner@@ ty , though , seems like k@@ f identity remin@@ der in@@ fin@@ ite different path tony could taken life . possibly along car joke involving in@@ fin@@ it@@ i made sense otherwise . a@@ a@@ and point brain fi@@ zz@@ le .\n",
      "i disappointed series . it lot cool graphic . the level detail went minimal , i always got feeling audience pat@@ ron@@ ized - - lot seemed this extremely cool going explain detail get anyway . let show pretty picture entertain . the host would drop interesting - sounding word spar@@ tic@@ les super - sym@@ me@@ try without attempt explaining . we look wi@@ ki@@ pe@@ dia . furthermore , i know quite bit super@@ string la@@ y@@ man i found explanation convoluted could much better . they could chosen much better example explain concept , instead , example used confusing ob@@ scu@@ red subject . ad@@ di@@ tionally , i got sick repeti@@ ti@@ veness . they could easily con@@ den@@ sed series one episode cut repeti@@ tion . they must shown clip qu@@ an@@ tum ca@@ f time . the host kept saying thing . i remember many time said the universe made tiny little vi@@ bra@@ ting string . it like trying brain@@ wa@@ sh u accep@@ ting super@@ string best thing since sli@@ ced b@@ read . finally , show ended unpleasant sense competition fer@@ mil@@ a@@ b cer@@ n , clearly bi@@ ased towards fer@@ mil@@ a@@ b . this supposed edu@@ cational program qu@@ an@@ tum physi@@ c , whether us better europe vice ver@@ sa ! i also felt part pat@@ ron@@ i@@ zing - - audi@@ en@@ ces need see conflict remain interested . please . give little credit . overall , thumb -\n",
      "the first minute tin@@ sel@@ town finger te@@ e@@ tering remote , po@@ ised flick around watch something else . the premise two writer , luck , living self - stor@@ age - space bin mildly amusing , , painfully bland . the introduction character , played joe pan@@ to@@ li@@ ano - big deal movie guy , life park sleep la@@ v@@ atory , offered hope i decided give minute . and kri@@ sty s@@ wan@@ sons introduction bud@@ ding film director bor@@ der@@ line ny@@ mp@@ hom@@ ani@@ ac , added bit sp@@ ice . her solid acting performance raised presence beyond welcome eye - candy inclu@@ sion . ultimately , obvious low - budget impact film poorly shot scene , stu@@ t@@ tured pace slapstick hand@@ ling certain moment . some favourite movie time low budget , whi@@ th@@ nail i one also deal guy dream , luck . however , money , actor save tin@@ sel@@ town terrible movie archi@@ ve nu@@ dge could cult movie archi@@ ve . i laughed loud scene involving joe pan@@ to@@ li@@ ano character . in particular , pen@@ ultimate scene terribly clichd , still funny , rich - - scre@@ wed - character house , story un@@ ra@@ vel@@ s towards final moment . i see tin@@ sel@@ town great stage play film - maker best transla@@ te celluloid , simply work i laughed loud scene one liner , i think first minute du@@ lled sen@@ s expectation degree i would laughed anything . unless stuck no@@ vel@@ ty coffee coa@@ ster , pick see bar@@ gain bu@@ cket .\n"
     ]
    }
   ],
   "source": [
    "# 分词后的数据长什么样,与分词前imdb_train.txt进行对比来理解，@@ 是分词的标记，如果一个单词被分开，就会加上@@\n",
    "!head ./imdb_train_bpe.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:32:25.720382Z",
     "start_time": "2025-01-24T06:32:25.705278Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:15.915880Z",
     "iopub.status.busy": "2025-01-24T14:22:15.915480Z",
     "iopub.status.idle": "2025-01-24T14:22:15.932527Z",
     "shell.execute_reply": "2025-01-24T14:22:15.931640Z",
     "shell.execute_reply.started": "2025-01-24T14:22:15.915845Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really liked Summerslam due look arena , cur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not many television show appeal quite many dif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The film quickly get major chase scene ever in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane Austen would definitely approve one ! Gwy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Expectations somewhat high I went see movie , ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           processed  label\n",
       "0  I really liked Summerslam due look arena , cur...      1\n",
       "1  Not many television show appeal quite many dif...      1\n",
       "2  The film quickly get major chase scene ever in...      0\n",
       "3  Jane Austen would definitely approve one ! Gwy...      1\n",
       "4  Expectations somewhat high I went see movie , ...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:33:21.681977Z",
     "start_time": "2025-01-24T06:33:20.743071Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:15.934172Z",
     "iopub.status.busy": "2025-01-24T14:22:15.933707Z",
     "iopub.status.idle": "2025-01-24T14:22:17.988058Z",
     "shell.execute_reply": "2025-01-24T14:22:17.987205Z",
     "shell.execute_reply.started": "2025-01-24T14:22:15.934129Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "subwords = []\n",
    "with open(\"imdb_train_bpe.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    for line in file.readlines():\n",
    "        subwords.append(line.strip())\n",
    "        \n",
    "with open(\"imdb_test_bpe.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    for line in file.readlines():\n",
    "        subwords.append(line.strip())\n",
    "        \n",
    "cleaned_df[\"subwords10k\"] = subwords # 保存分词后的结果\n",
    "cleaned_df[\"split\"] = [\"train\"] * 25000 + [\"test\"] * 25000 # 标记训练集和测试集\n",
    "cleaned_df.to_csv(\"imdb_subwords.csv\", index=False) #把分词后的结果保存到csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:35:50.272092Z",
     "start_time": "2025-01-24T06:35:49.368592Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:17.989376Z",
     "iopub.status.busy": "2025-01-24T14:22:17.989008Z",
     "iopub.status.idle": "2025-01-24T14:22:19.688815Z",
     "shell.execute_reply": "2025-01-24T14:22:19.688002Z",
     "shell.execute_reply.started": "2025-01-24T14:22:17.989346Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 随后加载数据集就从bpe分词的文件里加载\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, mode=\"train\"):\n",
    "        df = pd.read_csv(\"imdb_subwords.csv\").query(\"split == '{}'\".format(mode)) # 加载训练集或测试集，query语句筛选\n",
    "        self.texts = df[\"subwords10k\"].values # 评论文本\n",
    "        self.labels = df[\"label\"].values # 评论标签\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx].split(), self.labels[idx] # 返回文本和标签\n",
    "    \n",
    "    \n",
    "train_ds = IMDBDataset(\"train\")\n",
    "test_ds = IMDBDataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:35:55.250388Z",
     "start_time": "2025-01-24T06:35:55.246372Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:19.690328Z",
     "iopub.status.busy": "2025-01-24T14:22:19.689930Z",
     "iopub.status.idle": "2025-01-24T14:22:19.697894Z",
     "shell.execute_reply": "2025-01-24T14:22:19.697120Z",
     "shell.execute_reply.started": "2025-01-24T14:22:19.690297Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['i',\n",
       "  'really',\n",
       "  'liked',\n",
       "  'sum@@',\n",
       "  'mer@@',\n",
       "  'sla@@',\n",
       "  'm',\n",
       "  'due',\n",
       "  'look',\n",
       "  'a@@',\n",
       "  'ren@@',\n",
       "  'a',\n",
       "  ',',\n",
       "  'cur@@',\n",
       "  'tain',\n",
       "  'look',\n",
       "  'overall',\n",
       "  'interesting',\n",
       "  'reason',\n",
       "  '.',\n",
       "  'anyways',\n",
       "  ',',\n",
       "  'could',\n",
       "  'one',\n",
       "  'best',\n",
       "  'sum@@',\n",
       "  'mer@@',\n",
       "  'sla@@',\n",
       "  'm',\n",
       "  'ever',\n",
       "  'w@@',\n",
       "  'w@@',\n",
       "  'f',\n",
       "  'le@@',\n",
       "  'x',\n",
       "  'lu@@',\n",
       "  'ger',\n",
       "  'main',\n",
       "  'event',\n",
       "  'yo@@',\n",
       "  'ko@@',\n",
       "  'z@@',\n",
       "  'un@@',\n",
       "  'a',\n",
       "  ',',\n",
       "  'time',\n",
       "  'ok',\n",
       "  'huge',\n",
       "  'fat',\n",
       "  'man',\n",
       "  'v',\n",
       "  'strong',\n",
       "  'man',\n",
       "  'i',\n",
       "  'glad',\n",
       "  'time',\n",
       "  'changed',\n",
       "  '.',\n",
       "  'it',\n",
       "  'terrible',\n",
       "  'main',\n",
       "  'event',\n",
       "  'like',\n",
       "  'every',\n",
       "  'match',\n",
       "  'lu@@',\n",
       "  'ger',\n",
       "  'terrible',\n",
       "  '.',\n",
       "  'other',\n",
       "  'match',\n",
       "  'card',\n",
       "  'ra@@',\n",
       "  'z@@',\n",
       "  'or',\n",
       "  'ra@@',\n",
       "  'mon',\n",
       "  'v',\n",
       "  'ted',\n",
       "  'di@@',\n",
       "  'bi@@',\n",
       "  'ase',\n",
       "  ',',\n",
       "  'ste@@',\n",
       "  'in@@',\n",
       "  'er',\n",
       "  'brothers',\n",
       "  'v',\n",
       "  'hea@@',\n",
       "  'ven@@',\n",
       "  'ly',\n",
       "  'bo@@',\n",
       "  'dies',\n",
       "  ',',\n",
       "  'sha@@',\n",
       "  'wn',\n",
       "  'micha@@',\n",
       "  'els',\n",
       "  'v',\n",
       "  'cur@@',\n",
       "  't',\n",
       "  'h@@',\n",
       "  'ening',\n",
       "  ',',\n",
       "  'event',\n",
       "  'sha@@',\n",
       "  'wn',\n",
       "  'named',\n",
       "  'big',\n",
       "  'monster',\n",
       "  'body',\n",
       "  'guard',\n",
       "  'die@@',\n",
       "  'sel',\n",
       "  ',',\n",
       "  'irs',\n",
       "  'v',\n",
       "  '-',\n",
       "  '-',\n",
       "  'kid',\n",
       "  ',',\n",
       "  'bre@@',\n",
       "  't',\n",
       "  'hart',\n",
       "  'first',\n",
       "  'take',\n",
       "  'do@@',\n",
       "  'ink',\n",
       "  'take',\n",
       "  'jerry',\n",
       "  'law@@',\n",
       "  'ler',\n",
       "  'stuff',\n",
       "  'har@@',\n",
       "  'ts',\n",
       "  'law@@',\n",
       "  'ler',\n",
       "  'always',\n",
       "  'interesting',\n",
       "  ',',\n",
       "  'lu@@',\n",
       "  'd@@',\n",
       "  'vi@@',\n",
       "  'g',\n",
       "  'bor@@',\n",
       "  'ga',\n",
       "  'destroyed',\n",
       "  'marty',\n",
       "  'jan@@',\n",
       "  'ne@@',\n",
       "  'tty',\n",
       "  ',',\n",
       "  'under@@',\n",
       "  'taker',\n",
       "  'took',\n",
       "  'giant',\n",
       "  'gon@@',\n",
       "  'z@@',\n",
       "  'ale@@',\n",
       "  'z',\n",
       "  'another',\n",
       "  'terrible',\n",
       "  'match',\n",
       "  ',',\n",
       "  'the',\n",
       "  'smoking',\n",
       "  'g@@',\n",
       "  'unn@@',\n",
       "  's',\n",
       "  'tat@@',\n",
       "  'an@@',\n",
       "  'ka',\n",
       "  'took',\n",
       "  'ba@@',\n",
       "  'm',\n",
       "  'ba@@',\n",
       "  'm',\n",
       "  'bi@@',\n",
       "  'ge@@',\n",
       "  'low',\n",
       "  'head@@',\n",
       "  'shr@@',\n",
       "  'in@@',\n",
       "  'kers',\n",
       "  ',',\n",
       "  'yo@@',\n",
       "  'ko@@',\n",
       "  'z@@',\n",
       "  'un@@',\n",
       "  'a',\n",
       "  'def@@',\n",
       "  'ended',\n",
       "  'world',\n",
       "  'title',\n",
       "  'le@@',\n",
       "  'x',\n",
       "  'lu@@',\n",
       "  'ger',\n",
       "  'match',\n",
       "  'boring',\n",
       "  'terrible',\n",
       "  'ending',\n",
       "  '.',\n",
       "  'however',\n",
       "  'deserves'],\n",
       " 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造 word2idx 和 idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:36:22.298401Z",
     "start_time": "2025-01-24T06:36:22.295299Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:19.699010Z",
     "iopub.status.busy": "2025-01-24T14:22:19.698703Z",
     "iopub.status.idle": "2025-01-24T14:22:19.703834Z",
     "shell.execute_reply": "2025-01-24T14:22:19.703092Z",
     "shell.execute_reply.started": "2025-01-24T14:22:19.698985Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[PAD]', 1: '[BOS]', 2: '[UNK]', 3: '[EOS]'}\n"
     ]
    }
   ],
   "source": [
    "#载入词表，看下词表长度，词表就像英语字典\n",
    "word2idx = {\n",
    "    \"[PAD]\": 0,     # 填充 token\n",
    "    \"[BOS]\": 1,     # begin of sentence\n",
    "    \"[UNK]\": 2,     # 未知 token\n",
    "    \"[EOS]\": 3,     # end of sentence\n",
    "}\n",
    "idx2word = {value: key for key, value in word2idx.items()}\n",
    "print(idx2word)\n",
    "index = len(idx2word) # 词表长度，现在为4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:39:49.547123Z",
     "start_time": "2025-01-24T06:39:49.532122Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:19.704970Z",
     "iopub.status.busy": "2025-01-24T14:22:19.704654Z",
     "iopub.status.idle": "2025-01-24T14:22:19.723258Z",
     "shell.execute_reply": "2025-01-24T14:22:19.722620Z",
     "shell.execute_reply.started": "2025-01-24T14:22:19.704945Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8031/8031 [00:00<00:00, 848154.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 8035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 1  # 出现次数低于此的token舍弃\n",
    "with open(\"imdb_bpe_vocab\", \"r\", encoding=\"utf8\") as file:\n",
    "    for line in tqdm(file.readlines()):\n",
    "        token, counts = line.strip().split()\n",
    "        if int(counts) >= threshold:\n",
    "            word2idx[token] = index # 加入词表\n",
    "            idx2word[index] = token # 加入反向词典\n",
    "            index += 1\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "print(\"vocab_size: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:40:24.355459Z",
     "start_time": "2025-01-24T06:40:23.470404Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:19.724332Z",
     "iopub.status.busy": "2025-01-24T14:22:19.724023Z",
     "iopub.status.idle": "2025-01-24T14:22:21.406209Z",
     "shell.execute_reply": "2025-01-24T14:22:21.405428Z",
     "shell.execute_reply.started": "2025-01-24T14:22:19.724308Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 选择 max_length\n",
    "length_collect = {}\n",
    "for text, label in train_ds:\n",
    "    length = len(text)\n",
    "    length_collect[length] = length_collect.get(length, 0) + 1\n",
    "    \n",
    "MAX_LENGTH = 500\n",
    "plt.bar(length_collect.keys(), length_collect.values())\n",
    "plt.axvline(MAX_LENGTH, label=\"max length\", c=\"gray\", ls=\":\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:44:33.165369Z",
     "start_time": "2025-01-24T06:44:33.158363Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:21.407629Z",
     "iopub.status.busy": "2025-01-24T14:22:21.407237Z",
     "iopub.status.idle": "2025-01-24T14:22:21.421125Z",
     "shell.execute_reply": "2025-01-24T14:22:21.420448Z",
     "shell.execute_reply.started": "2025-01-24T14:22:21.407600Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text-------------------\n",
      "['hello', 'world']\n",
      "['i', 'really', 'liked', 'sum@@', 'mer@@', 'sla@@', 'm', 'due', 'look', 'a@@', 'ren@@', 'a', ',', 'cur@@']\n",
      "['this', 'is', 'a', 'test']\n",
      "indices---------------\n",
      "tensor([   1, 6513,  125,    3,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n",
      "tensor([   1,    6,   27,  514, 2230,  775, 1527,  146,  893,   65,   98,  666,\n",
      "          26,    5, 1571,    3])\n",
      "tensor([   1,   18,  395,   26, 1892,    3,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, word2idx, idx2word, max_length=500, pad_idx=0, bos_idx=1, eos_idx=3, unk_idx=2):\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.max_length = max_length\n",
    "        self.pad_idx = pad_idx\n",
    "        self.bos_idx = bos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.unk_idx = unk_idx\n",
    "    \n",
    "    def encode(self, text_list, padding_first=False):\n",
    "        \"\"\"如果padding_first == True，则padding加载前面，否则加载后面\"\"\"\n",
    "        max_length = min(self.max_length, 2 + max([len(text) for text in text_list]))\n",
    "        indices_list = []\n",
    "        for text in text_list:\n",
    "            indices = [self.bos_idx] + [self.word2idx.get(word, self.unk_idx) for word in text[:max_length-2]] + [self.eos_idx] #变为id，未登录词用unk_idx代替，句子前后加bos和eos\n",
    "            if padding_first: # padding加载前面\n",
    "                indices = [self.pad_idx] * (max_length - len(indices)) + indices\n",
    "            else:# padding加载后面\n",
    "                indices = indices + [self.pad_idx] * (max_length - len(indices))\n",
    "            indices_list.append(indices)\n",
    "        return torch.tensor(indices_list)\n",
    "    \n",
    "    \n",
    "    def decode(self, indices_list, remove_bos=True, remove_eos=True, remove_pad=True, split=False):\n",
    "        text_list = []\n",
    "        for indices in indices_list:\n",
    "            text = []\n",
    "            for index in indices:\n",
    "                word = self.idx2word.get(index, \"[UNK]\")\n",
    "                if remove_bos and word == \"[BOS]\":\n",
    "                    continue\n",
    "                if remove_eos and word == \"[EOS]\":\n",
    "                    break\n",
    "                if remove_pad and word == \"[PAD]\":\n",
    "                    break\n",
    "                text.append(word)\n",
    "            text_list.append(\" \".join(text) if not split else text)\n",
    "        return text_list\n",
    "    \n",
    "\n",
    "tokenizer = Tokenizer(word2idx=word2idx, idx2word=idx2word)\n",
    "raw_text = [\"hello world\".split(), \"i really liked sum@@ mer@@ sla@@ m due look a@@ ren@@ a , cur@@ \".split(), \"this is a test\".split()]\n",
    "indices = tokenizer.encode(raw_text, padding_first=False)\n",
    "\n",
    "print(\"raw text-------------------\")\n",
    "for raw in raw_text:\n",
    "    print(raw)\n",
    "print(\"indices---------------\")\n",
    "for index in indices:\n",
    "    print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:45:07.249166Z",
     "start_time": "2025-01-24T06:45:07.246093Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:21.422258Z",
     "iopub.status.busy": "2025-01-24T14:22:21.421944Z",
     "iopub.status.idle": "2025-01-24T14:22:21.427284Z",
     "shell.execute_reply": "2025-01-24T14:22:21.426419Z",
     "shell.execute_reply.started": "2025-01-24T14:22:21.422229Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sum@@'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[2230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:46:29.054028Z",
     "start_time": "2025-01-24T06:46:29.051008Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:21.428833Z",
     "iopub.status.busy": "2025-01-24T14:22:21.428308Z",
     "iopub.status.idle": "2025-01-24T14:22:21.434443Z",
     "shell.execute_reply": "2025-01-24T14:22:21.433496Z",
     "shell.execute_reply.started": "2025-01-24T14:22:21.428789Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decode text\n",
      "[BOS] hello world [EOS] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[BOS] i really liked sum@@ mer@@ sla@@ m due look a@@ ren@@ a , cur@@ [EOS]\n",
      "[BOS] this is a test [EOS] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "decode_text = tokenizer.decode(indices.tolist(), remove_bos=False, remove_eos=False, remove_pad=False)\n",
    "print(\"decode text\")\n",
    "for decode in decode_text:\n",
    "    print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:47:44.738465Z",
     "start_time": "2025-01-24T06:47:44.735674Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:21.435985Z",
     "iopub.status.busy": "2025-01-24T14:22:21.435546Z",
     "iopub.status.idle": "2025-01-24T14:22:21.441404Z",
     "shell.execute_reply": "2025-01-24T14:22:21.440539Z",
     "shell.execute_reply.started": "2025-01-24T14:22:21.435943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really liked summerslam due look arena , cur\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 输入字符串\n",
    "text = \"i really liked sum@@ mer@@ sla@@ m due look a@@ ren@@ a , cur@@ \"\n",
    "\n",
    "# 使用正则表达式替换 \"@@ \" 为空字符串\n",
    "cleaned_text = re.sub(r'@@\\s*', '', text)\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T06:51:25.299316Z",
     "start_time": "2025-01-24T06:51:25.295189Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:21.447000Z",
     "iopub.status.busy": "2025-01-24T14:22:21.446557Z",
     "iopub.status.idle": "2025-01-24T14:22:21.455146Z",
     "shell.execute_reply": "2025-01-24T14:22:21.454201Z",
     "shell.execute_reply.started": "2025-01-24T14:22:21.446957Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fct(batch):\n",
    "    \"\"\"\n",
    "    把字符串列表转化为tensor\n",
    "    :param batch:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    text_list = [item[0] for item in batch] #batch中每个item的第一个元素是text,是输入，类型为list\n",
    "    label_list = [item[1] for item in batch] #batch中每个item的第二个元素是label,是输出，类型为int\n",
    "    # 这里使用 padding first\n",
    "    text_list = tokenizer.encode(text_list, padding_first=True).to(dtype=torch.int)\n",
    "    return text_list, torch.tensor(label_list).reshape(-1, 1).to(dtype=torch.float)\n",
    "\n",
    "batch_size = 128\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fct)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:00:33.794767500Z",
     "start_time": "2024-07-30T07:00:33.778195400Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:21.456380Z",
     "iopub.status.busy": "2025-01-24T14:22:21.456060Z",
     "iopub.status.idle": "2025-01-24T14:22:21.473021Z",
     "shell.execute_reply": "2025-01-24T14:22:21.472362Z",
     "shell.execute_reply.started": "2025-01-24T14:22:21.456355Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================== 一层单向 LSTM ===================================\n",
      "            embeding.weight             paramerters num: 128560\n",
      "           lstm.weight_ih_l0            paramerters num: 4096\n",
      "           lstm.weight_hh_l0            paramerters num: 16384\n",
      "            lstm.bias_ih_l0             paramerters num: 256\n",
      "            lstm.bias_hh_l0             paramerters num: 256\n",
      "              layer.weight              paramerters num: 4096\n",
      "               layer.bias               paramerters num: 64\n",
      "               fc.weight                paramerters num: 64\n",
      "                fc.bias                 paramerters num: 1\n",
      "================================== 一层双向 LSTM ===================================\n",
      "            embeding.weight             paramerters num: 128560\n",
      "           lstm.weight_ih_l0            paramerters num: 4096\n",
      "           lstm.weight_hh_l0            paramerters num: 16384\n",
      "            lstm.bias_ih_l0             paramerters num: 256\n",
      "            lstm.bias_hh_l0             paramerters num: 256\n",
      "       lstm.weight_ih_l0_reverse        paramerters num: 4096\n",
      "       lstm.weight_hh_l0_reverse        paramerters num: 16384\n",
      "        lstm.bias_ih_l0_reverse         paramerters num: 256\n",
      "        lstm.bias_hh_l0_reverse         paramerters num: 256\n",
      "              layer.weight              paramerters num: 8192\n",
      "               layer.bias               paramerters num: 64\n",
      "               fc.weight                paramerters num: 64\n",
      "                fc.bias                 paramerters num: 1\n",
      "================================== 两层单向 LSTM ===================================\n",
      "            embeding.weight             paramerters num: 128560\n",
      "           lstm.weight_ih_l0            paramerters num: 4096\n",
      "           lstm.weight_hh_l0            paramerters num: 16384\n",
      "            lstm.bias_ih_l0             paramerters num: 256\n",
      "            lstm.bias_hh_l0             paramerters num: 256\n",
      "           lstm.weight_ih_l1            paramerters num: 16384\n",
      "           lstm.weight_hh_l1            paramerters num: 16384\n",
      "            lstm.bias_ih_l1             paramerters num: 256\n",
      "            lstm.bias_hh_l1             paramerters num: 256\n",
      "              layer.weight              paramerters num: 4096\n",
      "               layer.bias               paramerters num: 64\n",
      "               fc.weight                paramerters num: 64\n",
      "                fc.bias                 paramerters num: 1\n"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim=16, hidden_dim=64, vocab_size=vocab_size, num_layers=1, bidirectional=False):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embeding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.layer = nn.Linear(hidden_dim * (2 if bidirectional else 1), hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # [bs, seq length]\n",
    "        x = self.embeding(x)\n",
    "        # [bs, seq length, embedding_dim] -> shape [bs, seq length, hidden_dim ]\n",
    "        seq_output, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        x = seq_output[:, -1, :]\n",
    "        # 取最后一个时间步的输出 (这也是为什么要设置padding_first=True的原因)\n",
    "        x = self.layer(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "sample_inputs = torch.randint(0, vocab_size, (2, 128))\n",
    "    \n",
    "print(\"{:=^80}\".format(\" 一层单向 LSTM \"))       \n",
    "for key, value in LSTM().named_parameters():\n",
    "    print(f\"{key:^40}paramerters num: {np.prod(value.shape)}\")\n",
    "\n",
    "    \n",
    "print(\"{:=^80}\".format(\" 一层双向 LSTM \"))       \n",
    "for key, value in LSTM(bidirectional=True).named_parameters():\n",
    "    print(f\"{key:^40}paramerters num: {np.prod(value.shape)}\")\n",
    "\n",
    "    \n",
    "print(\"{:=^80}\".format(\" 两层单向 LSTM \"))       \n",
    "for key, value in LSTM(num_layers=2).named_parameters():\n",
    "    print(f\"{key:^40}paramerters num: {np.prod(value.shape)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:21.474446Z",
     "iopub.status.busy": "2025-01-24T14:22:21.474128Z",
     "iopub.status.idle": "2025-01-24T14:22:21.512131Z",
     "shell.execute_reply": "2025-01-24T14:22:21.511476Z",
     "shell.execute_reply.started": "2025-01-24T14:22:21.474420Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluating(model, dataloader, loss_fct):\n",
    "    loss_list = []\n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "    for datas, labels in dataloader:\n",
    "        datas = datas.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 前向计算\n",
    "        logits = model(datas)\n",
    "        loss = loss_fct(logits, labels)         # 验证集损失\n",
    "        loss_list.append(loss.item())\n",
    "        # 二分类\n",
    "        preds = logits > 0\n",
    "        pred_list.extend(preds.cpu().numpy().tolist())\n",
    "        label_list.extend(labels.cpu().numpy().tolist())\n",
    "        \n",
    "    acc = accuracy_score(label_list, pred_list)\n",
    "    return np.mean(loss_list), acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard 可视化\n",
    "\n",
    "\n",
    "训练过程中可以使用如下命令启动tensorboard服务。\n",
    "\n",
    "```shell\n",
    "tensorboard \\\n",
    "    --logdir=runs \\     # log 存放路径\n",
    "    --host 0.0.0.0 \\    # ip\n",
    "    --port 8848         # 端口\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:21.513523Z",
     "iopub.status.busy": "2025-01-24T14:22:21.512963Z",
     "iopub.status.idle": "2025-01-24T14:22:21.597141Z",
     "shell.execute_reply": "2025-01-24T14:22:21.596434Z",
     "shell.execute_reply.started": "2025-01-24T14:22:21.513496Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class TensorBoardCallback:\n",
    "    def __init__(self, log_dir, flush_secs=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            log_dir (str): dir to write log.\n",
    "            flush_secs (int, optional): write to dsk each flush_secs seconds. Defaults to 10.\n",
    "        \"\"\"\n",
    "        self.writer = SummaryWriter(log_dir=log_dir, flush_secs=flush_secs)\n",
    "\n",
    "    def draw_model(self, model, input_shape):\n",
    "        self.writer.add_graph(model, input_to_model=torch.randn(input_shape))\n",
    "        \n",
    "    def add_loss_scalars(self, step, loss, val_loss):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/loss\", \n",
    "            tag_scalar_dict={\"loss\": loss, \"val_loss\": val_loss},\n",
    "            global_step=step,\n",
    "            )\n",
    "        \n",
    "    def add_acc_scalars(self, step, acc, val_acc):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/accuracy\",\n",
    "            tag_scalar_dict={\"accuracy\": acc, \"val_accuracy\": val_acc},\n",
    "            global_step=step,\n",
    "        )\n",
    "        \n",
    "    def add_lr_scalars(self, step, learning_rate):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/learning_rate\",\n",
    "            tag_scalar_dict={\"learning_rate\": learning_rate},\n",
    "            global_step=step,\n",
    "            \n",
    "        )\n",
    "    \n",
    "    def __call__(self, step, **kwargs):\n",
    "        # add loss\n",
    "        loss = kwargs.pop(\"loss\", None)\n",
    "        val_loss = kwargs.pop(\"val_loss\", None)\n",
    "        if loss is not None and val_loss is not None:\n",
    "            self.add_loss_scalars(step, loss, val_loss)\n",
    "        # add acc\n",
    "        acc = kwargs.pop(\"acc\", None)\n",
    "        val_acc = kwargs.pop(\"val_acc\", None)\n",
    "        if acc is not None and val_acc is not None:\n",
    "            self.add_acc_scalars(step, acc, val_acc)\n",
    "        # add lr\n",
    "        learning_rate = kwargs.pop(\"lr\", None)\n",
    "        if learning_rate is not None:\n",
    "            self.add_lr_scalars(step, learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:21.598425Z",
     "iopub.status.busy": "2025-01-24T14:22:21.597977Z",
     "iopub.status.idle": "2025-01-24T14:22:21.604757Z",
     "shell.execute_reply": "2025-01-24T14:22:21.604040Z",
     "shell.execute_reply.started": "2025-01-24T14:22:21.598397Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SaveCheckpointsCallback:\n",
    "    def __init__(self, save_dir, save_step=5000, save_best_only=True):\n",
    "        \"\"\"\n",
    "        Save checkpoints each save_epoch epoch. \n",
    "        We save checkpoint by epoch in this implementation.\n",
    "        Usually, training scripts with pytorch evaluating model and save checkpoint by step.\n",
    "\n",
    "        Args:\n",
    "            save_dir (str): dir to save checkpoint\n",
    "            save_epoch (int, optional): the frequency to save checkpoint. Defaults to 1.\n",
    "            save_best_only (bool, optional): If True, only save the best model or save each model at every epoch.\n",
    "        \"\"\"\n",
    "        self.save_dir = save_dir\n",
    "        self.save_step = save_step\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best_metrics = -1\n",
    "        \n",
    "        # mkdir\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.mkdir(self.save_dir)\n",
    "        \n",
    "    def __call__(self, step, state_dict, metric=None):\n",
    "        if step % self.save_step > 0:\n",
    "            return\n",
    "        \n",
    "        if self.save_best_only:\n",
    "            assert metric is not None\n",
    "            if metric >= self.best_metrics:\n",
    "                # save checkpoints\n",
    "                torch.save(state_dict, os.path.join(self.save_dir, \"best.ckpt\"))\n",
    "                # update best metrics\n",
    "                self.best_metrics = metric\n",
    "        else:\n",
    "            torch.save(state_dict, os.path.join(self.save_dir, f\"{step}.ckpt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:21.606218Z",
     "iopub.status.busy": "2025-01-24T14:22:21.605908Z",
     "iopub.status.idle": "2025-01-24T14:22:21.611750Z",
     "shell.execute_reply": "2025-01-24T14:22:21.610833Z",
     "shell.execute_reply.started": "2025-01-24T14:22:21.606193Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopCallback:\n",
    "    def __init__(self, patience=5, min_delta=0.01):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            patience (int, optional): Number of epochs with no improvement after which training will be stopped.. Defaults to 5.\n",
    "            min_delta (float, optional): Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute \n",
    "                change of less than min_delta, will count as no improvement. Defaults to 0.01.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_metric = -1\n",
    "        self.counter = 0\n",
    "        \n",
    "    def __call__(self, metric):\n",
    "        if metric >= self.best_metric + self.min_delta:\n",
    "            # update best metric\n",
    "            self.best_metric = metric\n",
    "            # reset counter \n",
    "            self.counter = 0\n",
    "        else: \n",
    "            self.counter += 1\n",
    "            \n",
    "    @property\n",
    "    def early_stop(self):\n",
    "        return self.counter >= self.patience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T14:22:21.613172Z",
     "iopub.status.busy": "2025-01-24T14:22:21.612801Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 3132/3920 [02:15<00:18, 42.53it/s, epoch=15]"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "def training(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    epoch, \n",
    "    loss_fct, \n",
    "    optimizer, \n",
    "    tensorboard_callback=None,\n",
    "    save_ckpt_callback=None,\n",
    "    early_stop_callback=None,\n",
    "    eval_step=500,\n",
    "    ):\n",
    "    record_dict = {\n",
    "        \"train\": [],\n",
    "        \"val\": []\n",
    "    }\n",
    "    \n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    with tqdm(total=epoch * len(train_loader)) as pbar:\n",
    "        for epoch_id in range(epoch):\n",
    "            # training\n",
    "            for datas, labels in train_loader:\n",
    "                datas = datas.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # 梯度清空\n",
    "                optimizer.zero_grad()\n",
    "                # 模型前向计算\n",
    "                logits = model(datas)\n",
    "                # 计算损失\n",
    "                loss = loss_fct(logits, labels)\n",
    "                # 梯度回传\n",
    "                loss.backward()\n",
    "                # 调整优化器，包括学习率的变动等\n",
    "                optimizer.step()\n",
    "                preds = logits > 0\n",
    "            \n",
    "                acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())    \n",
    "                loss = loss.cpu().item()\n",
    "                # record\n",
    "                \n",
    "                record_dict[\"train\"].append({\n",
    "                    \"loss\": loss, \"acc\": acc, \"step\": global_step\n",
    "                })\n",
    "                \n",
    "                # evaluating\n",
    "                if global_step % eval_step == 0:\n",
    "                    model.eval()\n",
    "                    val_loss, val_acc = evaluating(model, val_loader, loss_fct)\n",
    "                    record_dict[\"val\"].append({\n",
    "                        \"loss\": val_loss, \"acc\": val_acc, \"step\": global_step\n",
    "                    })\n",
    "                    model.train()\n",
    "                    \n",
    "                    # 1. 使用 tensorboard 可视化\n",
    "                    if tensorboard_callback is not None:\n",
    "                        tensorboard_callback(\n",
    "                            global_step, \n",
    "                            loss=loss, val_loss=val_loss,\n",
    "                            acc=acc, val_acc=val_acc,\n",
    "                            lr=optimizer.param_groups[0][\"lr\"],\n",
    "                            )\n",
    "                \n",
    "                    # 2. 保存模型权重 save model checkpoint\n",
    "                    if save_ckpt_callback is not None:\n",
    "                        save_ckpt_callback(global_step, model.state_dict(), metric=val_acc)\n",
    "\n",
    "                    # 3. 早停 Early Stop\n",
    "                    if early_stop_callback is not None:\n",
    "                        early_stop_callback(val_acc)\n",
    "                        if early_stop_callback.early_stop:\n",
    "                            print(f\"Early stop at epoch {epoch_id} / global_step {global_step}\")\n",
    "                            return record_dict\n",
    "                    \n",
    "                # udate step\n",
    "                global_step += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"epoch\": epoch_id})\n",
    "        \n",
    "    return record_dict\n",
    "        \n",
    "\n",
    "epoch = 20\n",
    "\n",
    "model = LSTM()\n",
    "\n",
    "# 1. 定义损失函数 采用交叉熵损失 (但是二分类)\n",
    "loss_fct = F.binary_cross_entropy_with_logits\n",
    "# 2. 定义优化器 采用 adam\n",
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 1. tensorboard 可视化\n",
    "if not os.path.exists(\"runs\"):\n",
    "    os.mkdir(\"runs\")\n",
    "tensorboard_callback = TensorBoardCallback(\"runs/imdb-lstm-subword\")\n",
    "# tensorboard_callback.draw_model(model, [1, MAX_LENGTH])\n",
    "# 2. save best\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.makedirs(\"checkpoints\")\n",
    "save_ckpt_callback = SaveCheckpointsCallback(\"checkpoints/imdb-lstm-subword\", save_step=len(train_dl), save_best_only=True)\n",
    "# 3. early stop\n",
    "early_stop_callback = EarlyStopCallback(patience=10)\n",
    "\n",
    "model = model.to(device)\n",
    "record = training(\n",
    "    model, \n",
    "    train_dl, \n",
    "    test_dl, \n",
    "    epoch, \n",
    "    loss_fct, \n",
    "    optimizer, \n",
    "    tensorboard_callback=tensorboard_callback,\n",
    "    save_ckpt_callback=save_ckpt_callback,\n",
    "    early_stop_callback=early_stop_callback,\n",
    "    eval_step=len(train_dl)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画线要注意的是损失是不一定在零到1之间的\n",
    "def plot_learning_curves(record_dict, sample_step=500):\n",
    "    # build DataFrame\n",
    "    train_df = pd.DataFrame(record_dict[\"train\"]).set_index(\"step\").iloc[::sample_step]\n",
    "    val_df = pd.DataFrame(record_dict[\"val\"]).set_index(\"step\")\n",
    "\n",
    "    # plot\n",
    "    fig_num = len(train_df.columns)\n",
    "    fig, axs = plt.subplots(1, fig_num, figsize=(5 * fig_num, 5))\n",
    "    for idx, item in enumerate(train_df.columns):    \n",
    "        axs[idx].plot(train_df.index, train_df[item], label=f\"train_{item}\")\n",
    "        axs[idx].plot(val_df.index, val_df[item], label=f\"val_{item}\")\n",
    "        axs[idx].grid()\n",
    "        axs[idx].legend()\n",
    "        # axs[idx].set_xticks(range(0, train_df.index[-1], 5000))\n",
    "        # axs[idx].set_xticklabels(map(lambda x: f\"{int(x/1000)}k\", range(0, train_df.index[-1], 5000)))\n",
    "        axs[idx].set_xlabel(\"step\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(record, sample_step=10)  #横坐标是 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataload for evaluating\n",
    "\n",
    "# load checkpoints\n",
    "model.load_state_dict(torch.load(\"checkpoints/imdb-lstm-subword/best.ckpt\", map_location=\"cpu\"))\n",
    "\n",
    "model.eval()\n",
    "loss, acc = evaluating(model, test_dl, loss_fct)\n",
    "print(f\"loss:     {loss:.4f}\\naccuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
